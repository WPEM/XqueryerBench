{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ee87d86-7f01-462e-869a-3500c76eb66a",
   "metadata": {},
   "source": [
    "\n",
    "## Tutorial for Operating XQueryerBench\n",
    "\n",
    "This template illustrates the workflow for XQueryerBench.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6f1fb6",
   "metadata": {},
   "source": [
    "\n",
    "## Installation\n",
    "Clone our benchmark framework from the following repository to train your model:\n",
    "git clone https://github.com/WPEM/XqueryerBench.git\n",
    "\n",
    "cd XqueryerBench\n",
    "\n",
    "conda create -n XqueryerBench python=3.10\n",
    "\n",
    "conda activate XqueryerBench\n",
    "\n",
    "pip install torch==2.0.1 --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a03397d-10ea-49f8-a693-abdd72b8e467",
   "metadata": {},
   "source": [
    "## Training The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb294c9a",
   "metadata": {},
   "source": [
    "python main.py --datapath \"/data/zzn/scp\" --model \"PatchTST\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92df45fe",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "\n",
    "Ensure you have set the correct paths for your training and validation databases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e6820f",
   "metadata": {},
   "source": [
    "## Applying the Training Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c821fa",
   "metadata": {},
   "source": [
    "Step1 The dataset class for loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6914370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import ase.db\n",
    "import json\n",
    "from scipy.interpolate import interp1d\n",
    "import ase\n",
    "import random\n",
    "import os\n",
    "class ASEDataset(Dataset):\n",
    "    def __init__(self, db_paths,encode_element,train=False):\n",
    "        with open('../CGCNN_atom_emb.json' , 'r') as file:\n",
    "            self.cgcnn_emb = json.load(file)\n",
    "        self.db_paths = db_paths\n",
    "        self.train=train\n",
    "        self.encode_element = encode_element\n",
    "        self.dbs = [ase.db.connect(db_path) for db_path in db_paths]\n",
    "        print(\"Loaded data from:\", db_paths)\n",
    "\n",
    "    def __len__(self):\n",
    "        total_length = sum(len(db) for db in self.dbs)\n",
    "        return total_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        cumulative_length = 0\n",
    "        for i, db in enumerate(self.dbs):\n",
    "            if idx < cumulative_length + len(db):\n",
    "                # Adjust the index to the range of the current database\n",
    "                adjusted_idx = idx - cumulative_length\n",
    "                row = db.get(adjusted_idx + 1)  # ASE db indexing starts from 1\n",
    "                if self.encode_element:\n",
    "                    atoms = row.toatoms()\n",
    "                    element = self.random_remove_elements(set(atoms.get_chemical_symbols()))\n",
    "                    element_encode = self.symbol_to_atomic_number(element)\n",
    "                    element_value = []\n",
    "                    for code in element_encode:\n",
    "                        value = self.cgcnn_emb[str(code)]\n",
    "                        element_value.append(value)\n",
    "                    # mean pooling\n",
    "                    element_value=torch.mean(torch.tensor(element_value, dtype=torch.float32),dim=0)\n",
    "                # Extract relevant data from the row\n",
    "                #latt_dis = eval(getattr(row, 'latt_dis'))\n",
    "                if self.train:\n",
    "                    intensity = self.mixture( eval(getattr(row, 'intensity')) )\n",
    "                else:\n",
    "                    intensity = eval(getattr(row, 'intensity')) \n",
    "                id_num = getattr(row, 'Label')\n",
    "                \n",
    "                # Convert to tensors\n",
    "                #tensor_latt_dis = torch.tensor(latt_dis, dtype=torch.float32)\n",
    "                tensor_intensity = torch.tensor(intensity, dtype=torch.float32)\n",
    "                tensor_id = torch.tensor(id_num, dtype=torch.int64)\n",
    "                if self.encode_element:\n",
    "                    return {\n",
    "                        #'latt_dis': tensor_latt_dis,\n",
    "                        'intensity': tensor_intensity,\n",
    "                        'id': tensor_id,\n",
    "                        'element': element_value\n",
    "                    }\n",
    "                else:\n",
    "                    return {\n",
    "                        #'latt_dis': tensor_latt_dis,\n",
    "                        'intensity': tensor_intensity,\n",
    "                        'id': tensor_id,\n",
    "                        'element': torch.zeros(92, dtype=torch.int)\n",
    "                    }              \n",
    "            cumulative_length += len(db)\n",
    "\n",
    "    def random_remove_elements(self,lst):\n",
    "        n = len(lst)\n",
    "        num_elements_to_remove = random.randint(1, n)\n",
    "        indices_to_remove = random.sample(range(len(lst)), num_elements_to_remove)\n",
    "        new_lst = [item for index, item in enumerate(lst) if index not in indices_to_remove]   \n",
    "        return new_lst\n",
    "\n",
    "    def mixture(self,xrd,ratio=0.08):\n",
    "        num = random.randint(1, 100315)\n",
    "        _row = ase.db.connect(self.db_paths[0][:-8]+'val.db').get(num)\n",
    "        _int = eval(getattr(_row, 'intensity'))\n",
    "        result = np.array(xrd) * (1 - ratio) + np.array(_int) * ratio\n",
    "        return result\n",
    "\n",
    "\n",
    "    def symbol_to_atomic_number(self,symbol_list):\n",
    "        # Mapping of element symbols to atomic numbers\n",
    "        atomic_numbers = {\n",
    "            'H': 1, 'He': 2, 'Li': 3, 'Be': 4, 'B': 5,\n",
    "            'C': 6, 'N': 7, 'O': 8, 'F': 9, 'Ne': 10,\n",
    "            'Na': 11, 'Mg': 12, 'Al': 13, 'Si': 14, 'P': 15,\n",
    "            'S': 16, 'Cl': 17, 'Ar': 18, 'K': 19, 'Ca': 20,\n",
    "            'Sc': 21, 'Ti': 22, 'V': 23, 'Cr': 24, 'Mn': 25,\n",
    "            'Fe': 26, 'Co': 27, 'Ni': 28, 'Cu': 29, 'Zn': 30,\n",
    "            'Ga': 31, 'Ge': 32, 'As': 33, 'Se': 34, 'Br': 35,\n",
    "            'Kr': 36, 'Rb': 37, 'Sr': 38, 'Y': 39, 'Zr': 40,\n",
    "            'Nb': 41, 'Mo': 42, 'Tc': 43, 'Ru': 44, 'Rh': 45,\n",
    "            'Pd': 46, 'Ag': 47, 'Cd': 48, 'In': 49, 'Sn': 50,\n",
    "            'Sb': 51, 'Te': 52, 'I': 53, 'Xe': 54, 'Cs': 55,\n",
    "            'Ba': 56, 'La': 57, 'Ce': 58, 'Pr': 59, 'Nd': 60,\n",
    "            'Pm': 61, 'Sm': 62, 'Eu': 63, 'Gd': 64, 'Tb': 65,\n",
    "            'Dy': 66, 'Ho': 67, 'Er': 68, 'Tm': 69, 'Yb': 70,\n",
    "            'Lu': 71, 'Hf': 72, 'Ta': 73, 'W': 74, 'Re': 75,\n",
    "            'Os': 76, 'Ir': 77, 'Pt': 78, 'Au': 79, 'Hg': 80,\n",
    "            'Tl': 81, 'Pb': 82, 'Bi': 83, 'Po': 84, 'At': 85,\n",
    "            'Rn': 86, 'Fr': 87, 'Ra': 88, 'Ac': 89, 'Th': 90,\n",
    "            'Pa': 91, 'U': 92, 'Np': 93, 'Pu': 94, 'Am': 95,\n",
    "            'Cm': 96, 'Bk': 97, 'Cf': 98, 'Es': 99, 'Fm': 100,\n",
    "            'Md': 101, 'No': 102, 'Lr': 103, 'Rf': 104, 'Db': 105,\n",
    "            'Sg': 106, 'Bh': 107, 'Hs': 108, 'Mt': 109, 'Ds': 110,\n",
    "            'Rg': 111, 'Cn': 112, 'Nh': 113, 'Fl': 114, 'Mc': 115,\n",
    "            'Lv': 116, 'Ts': 117, 'Og': 118\n",
    "        }\n",
    "        \n",
    "\n",
    "        atomic_number_list = []\n",
    "\n",
    "        if symbol_list == []: atomic_number_list.append(0)\n",
    "        else:\n",
    "            for symbol in symbol_list:\n",
    "                if symbol in atomic_numbers:\n",
    "                    atomic_number_list.append(atomic_numbers[symbol])\n",
    "                else:\n",
    "                    atomic_number_list.append(0)  # Append None if symbol not in the dictionary\n",
    "            \n",
    "        return atomic_number_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cd943c",
   "metadata": {},
   "source": [
    "Step2:Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eb4e0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from: ['/data/zzn/scp/trainV.db']\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ASEDataset([os.path.join(\"/data/zzn/scp\", 'trainV.db')],encode_element=False,train=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e49b432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intensity': tensor([1.8063, 1.8182, 0.7750,  ..., 5.7430, 5.5384, 5.1612]),\n",
       " 'id': tensor(14),\n",
       " 'element': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        dtype=torch.int32)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeb6f40",
   "metadata": {},
   "source": [
    "Step3:Build a bidirectional GRU Model for crystal system classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c54a99dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BiGRU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiGRU, self).__init__()\n",
    "        self.hidden_size = 64\n",
    "        self.num_layers = 4\n",
    "        self.gru = nn.GRU(1, self.hidden_size, self.num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "\n",
    "        self.fc = nn.Linear(self.hidden_size*2, 100315)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.squeeze(1).unsqueeze(-1)\n",
    "        out, _ = self.gru(x)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6350d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zinanzheng/anaconda3/envs/xrdbench/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "device = torch.device('cuda:0')## Use GPU the train the model\n",
    "all_labels = []\n",
    "all_predicted = []\n",
    "model=BiGRU().to(device)## Move the model to the GPU\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00025)## Perform gradient descent using the Adam optimizer\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "419f24ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7838/7838 [14:57<00:00,  8.73it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for batch_index, data in enumerate(tqdm(train_loader)):\n",
    "    intensity,labels,element = data['intensity'].to(device), data['id'].to(device), data['element'].to(device)\n",
    "    # print(labels.max())\n",
    "    # break\n",
    "    intensity = intensity.unsqueeze(1)\n",
    "    element = element.unsqueeze(1)\n",
    "\n",
    "    batch_size=intensity.shape[0]\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    outputs = model(intensity)\n",
    "\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    all_labels.extend(labels.cpu().numpy())\n",
    "    all_predicted.extend(predicted.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ffd2e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zinanzheng/anaconda3/envs/xrdbench/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score,precision_score,recall_score,accuracy_score\n",
    "accuracy = accuracy_score(all_labels, all_predicted)\n",
    "macro_f1 = f1_score(all_labels, all_predicted, average='macro')\n",
    "macro_precision = precision_score(all_labels, all_predicted, average='macro')\n",
    "macro_recall = recall_score(all_labels, all_predicted, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87371106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 8.971739022080447e-06.\n",
      "The macro f1 score is 2.5995343623977543e-08.\n",
      "The macro precision is 1.3029645247752173e-08.\n",
      "The macro recall is 8.971739022080447e-06.\n"
     ]
    }
   ],
   "source": [
    "print(f'The accuracy is {accuracy}.')\n",
    "print(f'The macro f1 score is {macro_f1}.')\n",
    "print(f'The macro precision is {macro_precision}.')\n",
    "print(f'The macro recall is {macro_recall}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xrdbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
